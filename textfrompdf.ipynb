{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f95e86-6f68-41ba-8ca6-40f0a5728e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 92.2/232.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc0fceb3-aeac-4826-9ee2-39598b97a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "Lab Record  of  Big Data Analytics (CSF443)   BACHELOR OF TECHNOLOGY  in  COMPUTER SCIENCE AND ENGINEERING     Session 2024-25  Submitted to: - Submitted by: - Ms. Yashi Rastogi Name: Bhoomika Talwar Assistant Professor Roll No.: 210102511 School of Computing Sap ID: 1000016841 DIT University, Dehradun Class/Section: B P1    SCHOOL OF COMPUTING DIT UNIVERSITY, DEHRADUN (State Private University through State Legislature Act No. 10 of 2013 of Uttarakhand and approved by UGC) Mussoorie Diversion Road, Dehradun, Uttarakhand - 248009, India.  August-December 2024 \n",
      " \n",
      "      INDEX S.NO. NAME OF THE EXPERIMENTS CONDUCTION DATE INSTRUCTORâ€™s SIGNATURE 1 Installation of Hadoop.    2 Directory Management Tasks in Hadoop a. Create a directory in HDFS b. List the Contents of directory c. Remove a directory in HDFS    3 File Management Tasks in Hadoop a. Upload and download a file in HDFS b. See Contents of a File. c. Remove a file in HDFS.    4 File Transfer in Hadoop a. Copy a file from Source to destination. b. Move file from Source to Destination   5 Word Count Map Reduce program to understand MAP Reduce Paradigm.    6 Weather Report POC-Map Reduce Program to analyse time-temperature statistics and generate report with max/min temperature.   7 Implementing Matrix Multiplication with Hadoop Map Reduce.   8 Pig, Latin Scripts to sort, Group, Join Project and Filter the data.   9 Introduction to Weka tool to process data.   10 Use R to process data and visualize it using ggplot2   Experiment-01  \n",
      "  Objective:  Installation of Hadoop.   Code : # Step 1: Update the system and install Java sudo apt update sudo apt install default-jdk -y   # Step 2: Download and extract Hadoop wget https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz tar -xvzf hadoop-3.3.5.tar.gz sudo mv hadoop-3.3.5 /usr/local/hadoop   # Step 3: Set environment variables echo \"export HADOOP_HOME=/usr/local/hadoop\" >> ~/.bashrc echo \"export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\" >> ~/.bashrc source ~/.bashrc  # Step 4: Verify installation hadoop version  Output:  \n",
      "Experiment-02  \n",
      "    Objective: Directory Management Tasks in Hadoop a. Create a directory in HDFS b. List the Contents of directory c. Remove a directory in HDFS    Code:  # To create a directory in HDFS hdfs dfs -mkdir /example_directory # To List the contents of the Directory hdfs dfs -ls / # Remove a directory in HDFS hdfs dfs -rm -r /example_directory  Output:   \n",
      "Experiment-03  \n",
      "          Objective: File Management Tasks in Hadoop a. Upload and download a file in HDFS b. See Contents of a File. c. Remove a file in HDFS.   Code:  # To upload a file to HDFS  echo \"This is a sample file for HDFS testing.\" > sample_file.txt hdfs dfs -put sample_file.txt /example_directory # List the Contents of the Directory hdfs dfs -ls /example_directory # Remove a file in HDFS  hdfs dfs -rm /example_directory/sample_file.txt  Output:   \n",
      "Experiment -04  \n",
      "    Objective: File Transfer in Hadoop a. Copy a file from Source to destination. b. Move file from Source to Destination      Code:  # To copy a file from Source to Destination  hdfs dfs -cp /example_directory/sample_file.txt /destination_directory/ # Move a file from Source to Destination hdfs dfs -mv /destination_directory/sample_file.txt /final_directory/ Experiment -05  \n",
      "    Objective:  Word Count Map Reduce program to understand MAP Reduce Paradigm.  Code:  # Code of Word Count java program  import org.apache.hadoop.conf.Configuration;  import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper;  import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException;  import java.util.StringTokenizer; public class WordCount { public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {  private  final static  IntWritable  one = new IntWritable(1); private Text word = new Text();     public void map(Object key, Text value, Context context) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } }  public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> { private IntWritable result = new IntWritable();  public void reduce(Text key, Iterable<IntWritable> values, Context context)  \n",
      "  throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } }  public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"word count\"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } }    # Compilation and Execution  hadoop com.sun.tools.javac.Main WordCount.java jar cf wc.jar WordCount*.class hadoop jar wc.jar WordCount /example_directory/input /example_directory/output  Output:  \n",
      " \n",
      "  Experiment -06   Objective:  Weather Report POC-Map Reduce Program to analyse time-temperature statistics and generate report with max/min temperature. Code:  # Code for weather report java file  import org.apache.hadoop.conf.Configuration;  import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper;  import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException;  public class WeatherReport { public static class WeatherMapper extends Mapper<Object, Text, Text, IntWritable> { private final static IntWritable temperature = new IntWritable(); private Text time = new Text();  public void map(Object key, Text value, Context context) throws IOException, InterruptedException { String[] parts = value.toString().split(\",\"); if (parts.length == 2) { time.set(parts[0]); temperature.set(Integer.parseInt(parts[1])); context.write(time, temperature); } } }  public static class TemperatureReducer extends Reducer<Text, IntWritable, Text, IntWritable> { private IntWritable result = new IntWritable();  \n",
      "  public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException { int max = Integer.MIN_VALUE; int min = Integer.MAX_VALUE;  for (IntWritable val : values) { int temp = val.get(); if (temp > max) max = temp;  if (temp < min) min = temp; }   ,result); result.set(max); context.write(new Text(\"Max Temp at \" + key.toString()  result); } } result.set(min); context.write(new Text(\"Min Temp at \" + key.toString()),  public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"weather report\"); job.setJarByClass(WeatherReport.class); job.setMapperClass(WeatherMapper.class); job.setReducerClass(TemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } }  # Hadoop Code for Compile and Run hadoop com.sun.tools.javac.Main WeatherReport.java jar cf weather_report.jar WeatherReport*.class hadoop jar weather_report.jar WeatherReport /input/weather_data.csv /output/weather_report  Output:  \n",
      " \n",
      "  Experiment -07   Objective:  Implementing Matrix Multiplication with Hadoop Map Reduce.  Code:  # Code of the program matrix multiplication java  import org.apache.hadoop.conf.Configuration;  import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable;  import org.apache.hadoop.io.Text;  import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper;  import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; public class MatrixMultiplication { public static class MatrixMapper extends Mapper<Object, Text, Text, Text> { public void map(Object key, Text value, Context context) throws IOException, InterruptedException { String[] values = value.toString().split(\"\\t\"); String matrixData = values[0]; String rowColValue = values[1]; String[] matrixVal = rowColValue.split(\",\"); String row = matrixVal[0]; String col = matrixVal[1]; String val = matrixVal[2];  context.write(new Text(row + \",\" + col), new Text(matrixData + \",\" + val)); } }  public static class MatrixReducer extends Reducer<Text, Text, Text, IntWritable> { private IntWritable result = new IntWritable();  public void reduce(Text key, Iterable<Text> values, Context context)  \n",
      "  throws IOException, InterruptedException { int sum = 0; for (Text val : values) { String[] parts = val.toString().split(\",\"); int matrixVal = Integer.parseInt(parts[1]); sum += matrixVal; }  result.set(sum); context.write(key, result); } }  public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \"matrix multiplication\"); job.setJarByClass(MatrixMultiplication.class); job.setMapperClass(MatrixMapper.class); job.setReducerClass(MatrixReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } }   # Hadoop code to Compile and Run  hadoop com.sun.tools.javac.Main MatrixMultiplication.java jar cf matrix_multiplication.jar MatrixMultiplication*.class hadoop jar matrix_multiplication.jar MatrixMultiplication /input/matrix_a.txt /input/matrix_b.txt /output  Output:  \n",
      " \n",
      "  Experiment -08   Objective:  Pig, Latin Scripts to sort, Group, Join Project and Filter the data.  Code:  # Pig Script to Sort, Group, Join, Project and Filter  \n",
      "  # Execution  pig experiment.pig   Output # Input -- Load data data = LOAD 'input/data.txt' USING PigStorage(',') AS (id:int, name:chararray, age:int, salary:int);  -- Sorting the data by age sorted_data = ORDER data BY age DESC;  -- Grouping by age grouped_data = GROUP data BY age;  -- Projecting certain fields projected_data = FOREACH grouped_data GENERATE group AS age, data.name, data.salary;  -- Filtering data where age is greater than 30 filtered_data = FILTER projected_data BY age > 30;  -- Store the result STORE filtered_data INTO 'output/processed_data';  \n",
      "  \n",
      "  # Output  \n",
      "Experiment -09  \n",
      "    Objective:  Introduction to Weka tool to process data.  Code:  \n",
      " \n",
      "Experiment -10  \n",
      "    Objective:  Use R to process data and visualize it using ggplot2  Code:  # Code of the Program  \n",
      "  Output: # Load necessary libraries library(ggplot2)  # Load a sample dataset data(iris)  # Create a scatter plot of Sepal.Length vs Sepal.Width ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species)) + theme_minimal() + labs(title = \"Sepal Length vs Sepal Width in Iris Dataset\", x = \"Sepal Length\", y = \"Sepal Width\")  \n",
      "  \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "pdf_path = \"Aagam_jain_1000016518_BigData.pdf\"\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "if extracted_text:\n",
    "    print(\"Extracted Text:\")\n",
    "    print(extracted_text)\n",
    "else:\n",
    "    print(\"No text extracted or PDF might be empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f8f2aa-e171-4ad7-b694-a47c5afc3e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
